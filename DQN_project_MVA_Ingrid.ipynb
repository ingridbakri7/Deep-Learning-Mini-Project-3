{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You may need to install [OpenCV](https://pypi.python.org/pypi/opencv-python) and [scikit-video](http://www.scikit-video.org/stable/).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "import skvideo.io\n",
    "import cv2\n",
    "import json\n",
    "import random as rd\n",
    "from collections import deque\n",
    "\n",
    "from keras.models import Sequential,model_from_json\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import sgd, Adam\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation, AveragePooling2D,Reshape,BatchNormalization, Flatten, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MiniProject #3: Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Notations__: $E_p$ is the expectation under probability $p$. Please justify each of your answer and widely comment your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a reinforcement learning algorithm, we modelize each step $t$ as an action $a_t$ obtained from a state $s_t$, i.e. $\\{(a_{t},s_{t})_{t\\leq T}\\}$ having the Markov property. We consider a discount factor $\\gamma \\in [0,1]$ that ensures convergence. The goal is to find among all the policies $\\pi$, one that maximizes the expected reward:\n",
    "\n",
    "\\begin{equation*}\n",
    "R(\\pi)=\\sum_{t\\leq T}E_{p^{\\pi}}[\\gamma^t r(s_{t},a_{t})] \\> ,\n",
    "\\end{equation*}\n",
    "\n",
    "where: \n",
    "\\begin{equation*}p^{\\pi}(a_{0},a_{1},s_{1},...,a_{T},s_{T})=p(a_{0})\\prod_{t=1}^{T}\\pi(a_{t}|s_{t})p(s_{t+1}|s_{t},a_{t}) \\> .\n",
    "\\end{equation*}\n",
    "\n",
    "We note the $Q$-function:\n",
    "\n",
    "\\begin{equation*}Q^\\pi(s,a)=E_{p^{\\pi}}[\\sum_{t\\leq T}\\gamma^{t}r(s_{t},a_{t})|s_{0}=s,a_{0}=a] \\> .\n",
    "\\end{equation*}\n",
    "\n",
    "Thus, the optimal Q function is:\n",
    "\\begin{equation*}\n",
    "Q^*(s,a)=\\max_{\\pi}Q^\\pi(s,a) \\> .\n",
    "\\end{equation*}\n",
    "\n",
    "In this project, we will apply the deep reinforcement learning techniques to a simple game: an agent will have to learn from scratch a policy that will permit it maximizing a reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The environment, the agent and the game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Environment``` is an abstract class that represents the states, rewards, and actions to obtain the new state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def act(self, act):\n",
    "        \"\"\"\n",
    "        One can act on the environment and obtain its reaction:\n",
    "        - the new state\n",
    "        - the reward of the new state\n",
    "        - should we continue the game?\n",
    "\n",
    "        :return: state, reward, game_over\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reinitialize the environment to a random state and returns\n",
    "        the original state\n",
    "\n",
    "        :return: state\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def draw(self):\n",
    "        \"\"\"\n",
    "        Visualize in the console or graphically the current state\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method ```act``` allows to act on the environment at a given state $s_t$ (stored internally), via action $a_t$. The method will return the new state $s_{t+1}$, the reward $r(s_{t},a_{t})$ and determines if $t\\leq T$ (*game_over*).\n",
    "\n",
    "The method ```reset``` simply reinitializes the environment to a random state $s_0$.\n",
    "\n",
    "The method ```draw``` displays the current state $s_t$ (this is useful to check the behavior of the Agent).\n",
    "\n",
    "We modelize $s_t$ as a tensor, while $a_t$ is an integer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the ```Agent``` is to interact with the ```Environment``` by proposing actions $a_t$ obtained from a given state $s_t$ to attempt to maximize its __reward__ $r(s_t,a_t)$. We propose the following abstract class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, epsilon=0.1, n_action=4):\n",
    "        self.epsilon = epsilon\n",
    "        self.n_action = n_action\n",
    "    \n",
    "    def set_epsilon(self,e):\n",
    "        self.epsilon = e\n",
    "    \n",
    "    def get_epsilon(self):\n",
    "        return self.epsilon\n",
    "\n",
    "    def act(self,s,train=True):\n",
    "        \"\"\" This function should return the next action to do:\n",
    "        an integer between 0 and 4 (not included) with a random exploration of epsilon\"\"\"\n",
    "        if train:\n",
    "            if np.random.rand() <= self.epsilon:\n",
    "                a = np.random.randint(0, self.n_action, size=1)[0]\n",
    "            else:\n",
    "                a = self.learned_act(s)\n",
    "        else: # in some cases, this can improve the performance.. remove it if poor performances\n",
    "            a = self.learned_act(s)\n",
    "\n",
    "        return a\n",
    "\n",
    "    def learned_act(self,s):\n",
    "        \"\"\" Act via the policy of the agent, from a given state s\n",
    "        it proposes an action a\"\"\"\n",
    "        pass\n",
    "\n",
    "    def reinforce(self, s, n_s, a, r, game_over_):\n",
    "        \"\"\" This function is the core of the learning algorithm. \n",
    "        It takes as an input the current state s_, the next state n_s_\n",
    "        the action a_ used to move from s_ to n_s_ and the reward r_.\n",
    "        \n",
    "        Its goal is to learn a policy.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\" This function returns basic stats if applicable: the\n",
    "        loss and/or the model\"\"\"\n",
    "        pass\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\" This function allows to restore a model\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "__Question 1__:\n",
    "Explain the function act. Why is ```epsilon``` essential?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function ```act```: Given a state in a certain environment, this function will determine what will the next action be, based on a certain policy. However, a learned action will not take into consideration the probability of having a random action. By doing exploration it will check all the possible options and then choose the best one. This is where $\\epsilon$ has a role. It works as follows: A value is generated randomly, if it is less than $\\epsilon$, a random action is chosen, otherwise, the optimal max Q value will be chosen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### The Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```Agent``` and the ```Environment``` work in an interlaced way as in the following (take some time to understand this code as it is the core of the project)\n",
    "\n",
    "```python\n",
    "\n",
    "epoch = 300\n",
    "env = Environment()\n",
    "agent = Agent()\n",
    "\n",
    "\n",
    "# Number of won games\n",
    "score = 0\n",
    "loss = 0\n",
    "\n",
    "\n",
    "for e in range(epoch):\n",
    "    # At each epoch, we restart to a fresh game and get the initial state\n",
    "    state = env.reset()\n",
    "    # This assumes that the games will end\n",
    "    game_over = False\n",
    "\n",
    "    win = 0\n",
    "    lose = 0\n",
    "    \n",
    "    while not game_over:\n",
    "        # The agent performs an action\n",
    "        action = agent.act(state)\n",
    "\n",
    "        # Apply an action to the environment, get the next state, the reward\n",
    "        # and if the games end\n",
    "        prev_state = state\n",
    "        state, reward, game_over = env.act(action)\n",
    "\n",
    "        # Update the counters\n",
    "        if reward > 0:\n",
    "            win = win + reward\n",
    "        if reward < 0:\n",
    "            lose = lose -reward\n",
    "\n",
    "        # Apply the reinforcement strategy\n",
    "        loss = agent.reinforce(prev_state, state,  action, reward, game_over)\n",
    "\n",
    "    # Save as a mp4\n",
    "    if e % 10 == 0:\n",
    "        env.draw(e)\n",
    "\n",
    "    # Update stats\n",
    "    score += win-lose\n",
    "\n",
    "    print(\"Epoch {:03d}/{:03d} | Loss {:.4f} | Win/lose count {}/{} ({})\"\n",
    "          .format(e, epoch, loss, win, lose, win-lose))\n",
    "    agent.save()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The game, *eat cheese*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A rat runs on an island and tries to eat as much as possible. The island is subdivided into $N\\times N$ cells, in which there are cheese (+0.5) and poisonous cells (-1). The rat has a visibility of 2 cells (thus it can see $5^2$ cells). The rat is given a time $T$ to accumulate as much food as possible. It can perform 4 actions: going up, down, left, right. \n",
    "\n",
    "The goal is to code an agent to solve this task that will learn by trial and error. We propose the following environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    def __init__(self, grid_size=10, max_time=500, temperature=0.1):\n",
    "        grid_size = grid_size+4\n",
    "        self.grid_size = grid_size\n",
    "        self.max_time = max_time\n",
    "        self.temperature = temperature\n",
    "\n",
    "        #board on which one plays\n",
    "        self.board = np.zeros((grid_size,grid_size))\n",
    "        self.position = np.zeros((grid_size,grid_size))\n",
    "\n",
    "        # coordinate of the cat\n",
    "        self.x = 0\n",
    "        self.y = 1\n",
    "\n",
    "        # self time\n",
    "        self.t = 0\n",
    "\n",
    "        self.scale=16\n",
    "\n",
    "        self.to_draw = np.zeros((max_time+2, grid_size*self.scale, grid_size*self.scale, 3))\n",
    "\n",
    "\n",
    "    def draw(self,e):\n",
    "        skvideo.io.vwrite(str(e) + '.mp4', self.to_draw)\n",
    "\n",
    "    def get_frame(self,t):\n",
    "        b = np.zeros((self.grid_size,self.grid_size,3))+128\n",
    "        b[self.board>0,0] = 256\n",
    "        b[self.board < 0, 2] = 256\n",
    "        b[self.x,self.y,:]=256\n",
    "        b[-2:,:,:]=0\n",
    "        b[:,-2:,:]=0\n",
    "        b[:2,:,:]=0\n",
    "        b[:,:2,:]=0\n",
    "        \n",
    "        b =  cv2.resize(b, None, fx=self.scale, fy=self.scale, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        self.to_draw[t,:,:,:]=b\n",
    "\n",
    "\n",
    "    def act(self, action):\n",
    "        \"\"\"This function returns the new state, reward and decides if the\n",
    "        game ends.\"\"\"\n",
    "\n",
    "        self.get_frame(int(self.t))\n",
    "\n",
    "        self.position = np.zeros((self.grid_size, self.grid_size))\n",
    "\n",
    "        self.position[0:2,:]= -1\n",
    "        self.position[:,0:2] = -1\n",
    "        self.position[-2:, :] = -1\n",
    "        self.position[:, -2:] = -1\n",
    "\n",
    "        self.position[self.x, self.y] = 1\n",
    "        \n",
    "        if action == 0:\n",
    "            if self.x == self.grid_size-3:\n",
    "                self.x = self.x-1\n",
    "            else:\n",
    "                self.x = self.x + 1\n",
    "        elif action == 1:\n",
    "            if self.x == 2:\n",
    "                self.x = self.x+1\n",
    "            else:\n",
    "                self.x = self.x-1\n",
    "        elif action == 2:\n",
    "            if self.y == self.grid_size - 3:\n",
    "                self.y = self.y - 1\n",
    "            else:\n",
    "                self.y = self.y + 1\n",
    "        elif action == 3:\n",
    "            if self.y == 2:\n",
    "                self.y = self.y + 1\n",
    "            else:\n",
    "                self.y = self.y - 1\n",
    "        else:\n",
    "            RuntimeError('Error: action not recognized')\n",
    "\n",
    "        self.t = self.t + 1\n",
    "        reward = self.board[self.x, self.y]\n",
    "        self.board[self.x, self.y] = 0\n",
    "        game_over = self.t > self.max_time\n",
    "        state = np.concatenate((self.board.reshape(self.grid_size, self.grid_size,1),\n",
    "                                self.position.reshape(self.grid_size, self.grid_size,1)),axis=2)\n",
    "        state = state[self.x-2:self.x+3,self.y-2:self.y+3,:]\n",
    "\n",
    "        return state, reward, game_over\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"This function resets the game and returns the initial state\"\"\"\n",
    "\n",
    "        self.x = np.random.randint(3, self.grid_size-3, size=1)[0]\n",
    "        self.y = np.random.randint(3, self.grid_size-3, size=1)[0]\n",
    "\n",
    "\n",
    "        bonus = 0.5*np.random.binomial(1,self.temperature,size=self.grid_size**2)\n",
    "        bonus = bonus.reshape(self.grid_size,self.grid_size)\n",
    "\n",
    "        malus = -1.0*np.random.binomial(1,self.temperature,size=self.grid_size**2)\n",
    "        malus = malus.reshape(self.grid_size, self.grid_size)\n",
    "\n",
    "        self.to_draw = np.zeros((self.max_time+2, self.grid_size*self.scale, self.grid_size*self.scale, 3))\n",
    "\n",
    "\n",
    "        malus[bonus>0]=0\n",
    "\n",
    "        self.board = bonus + malus\n",
    "\n",
    "        self.position = np.zeros((self.grid_size, self.grid_size))\n",
    "        self.position[0:2,:]= -1\n",
    "        self.position[:,0:2] = -1\n",
    "        self.position[-2:, :] = -1\n",
    "        self.position[:, -2:] = -1\n",
    "        self.board[self.x,self.y] = 0\n",
    "        self.t = 0\n",
    "\n",
    "        state = np.concatenate((self.board.reshape(self.grid_size, self.grid_size,1),\n",
    "                                self.position.reshape(self.grid_size, self.grid_size,1)),axis=2)\n",
    "\n",
    "        state = state[self.x - 2:self.x + 3, self.y - 2:self.y + 3, :]\n",
    "        \n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following elements are important because they correspond to the hyper parameters for this project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "size = 13\n",
    "T=200\n",
    "temperature=0.3\n",
    "epochs_train=20 # set small when debugging\n",
    "epochs_test=20 # set small when debugging\n",
    "\n",
    "# display videos\n",
    "def display_videos(name):\n",
    "    video = io.open(name, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    return '''<video alt=\"test\" controls>\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 2__ Explain the use of the arrays ```position``` and ```board```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```position``` : Represents the current position of the rat on the board.\n",
    "\n",
    "```board``` : represents all the possible cells, this translates to the fact that ```board``` is a representation of all the possible states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "__Question 3__ Implement a random Agent (only ```learned_act``` needs to be implemented):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(Agent):\n",
    "    def __init__(self):\n",
    "        super(RandomAgent, self).__init__()\n",
    "        pass\n",
    "\n",
    "    def learned_act(self, s):\n",
    "        return rd.randint(0,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "__Question 4__ Visualize the game moves. You need to fill in the following function for the evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(agent,env,epochs,prefix=''):\n",
    "    # Number of won games\n",
    "    score = 0\n",
    "        \n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # At each epoch, we restart to a fresh game and get the initial state\n",
    "        state = env.reset()\n",
    "        # This assumes that the games will end\n",
    "        game_over = False\n",
    "\n",
    "        win = 0\n",
    "        lose = 0\n",
    "\n",
    "        while not game_over:\n",
    "            # The agent performs an action\n",
    "            action = agent.learned_act(state)\n",
    "\n",
    "            # Apply an action to the environment, get the next state, the reward\n",
    "            # and if the games end\n",
    "            prev_state = state\n",
    "            state, reward, game_over = env.act(action)\n",
    "\n",
    "            # Update the counters\n",
    "            if reward > 0:\n",
    "                win = win + reward\n",
    "            if reward < 0:\n",
    "                lose = lose -reward\n",
    "\n",
    "        # Save as a mp4\n",
    "        env.draw(prefix+str(e))\n",
    "\n",
    "        # Update stats\n",
    "        score = score + win-lose\n",
    "\n",
    "        print(\"Win/lose count {}/{}. Average score ({})\"\n",
    "              .format(win, lose, score/(1+e)))\n",
    "    print('Final score: '+str(score/epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win/lose count 17.0/18.0. Average score (-1.0)\n",
      "Win/lose count 9.5/17.0. Average score (-4.25)\n",
      "Win/lose count 6.0/6.0. Average score (-2.8333333333333335)\n",
      "Win/lose count 13.0/20.0. Average score (-3.875)\n",
      "Win/lose count 7.0/10.0. Average score (-3.7)\n",
      "Win/lose count 11.0/14.0. Average score (-3.5833333333333335)\n",
      "Win/lose count 7.5/15.0. Average score (-4.142857142857143)\n",
      "Win/lose count 8.5/14.0. Average score (-4.3125)\n",
      "Win/lose count 9.5/31.0. Average score (-6.222222222222222)\n",
      "Win/lose count 13.0/15.0. Average score (-5.8)\n",
      "Win/lose count 7.5/12.0. Average score (-5.681818181818182)\n",
      "Win/lose count 10.0/13.0. Average score (-5.458333333333333)\n",
      "Win/lose count 10.5/14.0. Average score (-5.3076923076923075)\n",
      "Win/lose count 15.5/16.0. Average score (-4.964285714285714)\n",
      "Win/lose count 9.5/17.0. Average score (-5.133333333333334)\n",
      "Win/lose count 10.5/14.0. Average score (-5.03125)\n",
      "Win/lose count 5.5/12.0. Average score (-5.117647058823529)\n",
      "Win/lose count 15.5/14.0. Average score (-4.75)\n",
      "Win/lose count 15.5/11.0. Average score (-4.2631578947368425)\n",
      "Win/lose count 9.0/14.0. Average score (-4.3)\n",
      "Final score: -4.3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video alt=\"test\" controls>\n",
       "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
       "             </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the game\n",
    "env = Environment(grid_size=size, max_time=T,temperature=temperature)\n",
    "\n",
    "# Initialize the agent!\n",
    "agent = RandomAgent()\n",
    "\n",
    "test(agent,env,epochs_test,prefix='random')\n",
    "HTML(display_videos('random0.mp4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us assume here that $T=\\infty$.\n",
    "\n",
    "***\n",
    "__Question 5__ Let $\\pi$ be a policy, show that:\n",
    "\n",
    "\\begin{equation*}\n",
    "Q^{\\pi}(s,a)=E_{(s',a')\\sim p(.|s,a)}[r(s,a)+\\gamma Q^{\\pi}(s',a')]\n",
    "\\end{equation*}\n",
    "\n",
    "Then, show that for the optimal policy $\\pi^*$ (we assume its existence), the following holds: \n",
    "\n",
    "\\begin{equation*}\n",
    "Q^{*}(s,a)=E_{s'\\sim \\pi^*(.|s,a)}[r(s,a)+\\gamma\\max_{a'}Q^{*}(s',a')].\n",
    "\\end{equation*}\n",
    "Finally, deduce that a plausible objective is:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathcal{L}(\\theta)=E_{s' \\sim \\pi^*(.|s,a)}\\Vert r+\\gamma\\max\\max_{a'}Q(s',a',\\theta)-Q(s,a,\\theta)\\Vert^{2}.\n",
    "\\end{equation*}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**\n",
    "\n",
    "The bellman equation is given as follows:\n",
    "\\begin{equation*}\n",
    "Q^{\\pi}(s,a)  = E_{\\pi}[\\Sigma^{\\infty}_{t=0} \\gamma^t r_t(s_t, \\pi(s_t),s_{t+1}) | s_t = s,a_t = a] \\\\\n",
    "\\end{equation*}\n",
    "\n",
    "Using the Markov Property, we can take the first element of the sum outside, we will be left with:\n",
    "\n",
    "\\begin{align*}\n",
    "Q^{\\pi}(s_0,a_0) & = E_{\\pi}[r_0(s_0,a_0,s_1)+\\Sigma^{\\infty}_{t=1} \\gamma^t r_t (s_t,\\pi(s_t),s_{t+1}) | s_0,a_0 ] \\\\\n",
    "& = E_{\\pi}[r_0(s_0,a_0,s_1) + \\gamma E_{\\pi} \\{ \\Sigma_{t=1}^{\\infty} \\gamma^{t-1} r_t (s_t,\\pi(s_t),s_{t+1} | s_1,a_1 \\} s_0,a_0 ] \\\\\n",
    "& = E_{\\pi} [r_0 (s_0,a_0,s_1)+\\gamma Q^{\\pi}(s_1,a_1)] \\\\\n",
    "& = E_{(s',a') \\sim p(.|s,a)} [r(s,a)+\\gamma Q^{\\pi}(s',a')]\n",
    "\\end{align*}\n",
    "\n",
    "Given an ultimate policy $\\pi^*$ , the optimal Q function can be written as:\n",
    "\n",
    "\\begin{equation*}\n",
    "Q^*(s,a)=\\max_{\\pi}Q^\\pi(s,a) \\> \n",
    "\\end{equation*}\n",
    "\n",
    "Using the previous equation that we derived in the first step:\n",
    "\n",
    "\\begin{equation*}\n",
    "Q^*(s,a)= \\max_{\\pi} E_{(s',a')\\sim p(.|s,a)}[r(s,a)+\\gamma Q^{\\pi}(s',a')] \\> \n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "Q^*(s,a)= E_{s'\\sim \\pi^*(.|s,a)}[r(s,a)+\\gamma Q^{\\pi^*}(s',a')] \\> \n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "Q^*(s,a)= E_{s'\\sim \\pi^*(.|s,a)}[r(s,a)+\\gamma\\max_{a'}Q^{*}(s',a')].\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\n",
    "Given the loss function $\\mathcal{L}(\\theta)= [Q^*(s,a,\\theta) - Q(s,a,\\theta)]^2$\n",
    "\n",
    "$$\\mathcal{L}(\\theta) = [E_{s'\\sim \\pi^*(.|s,a,s')}[r(s,a,s')+\\gamma\\max_{a'}Q^{*}(s',a,\\theta)] - Q(s,a,\\theta)]^2$$\n",
    "$$ {\\mathcal{L}(\\theta)=E_{s' \\sim \\pi^*(.|s,a)}\\Vert r+\\gamma\\max\\max_{a'}Q(s',a',\\theta)-Q(s,a,\\theta)\\Vert^{2}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "The DQN-learning algorithm relies on these derivations to train the parameters $\\theta$ of a Deep Neural Network:\n",
    "\n",
    "1. At the state $s_t$, select the action $a_t$ with best reward using $Q_t$ and store the results;\n",
    "\n",
    "2. Obtain the new state $s_{t+1}$ from the environment $p$;\n",
    "\n",
    "3. Store $(s_t,a_t,s_{t+1})$;\n",
    "\n",
    "4. Obtain $Q_{t+1}$ by minimizing  $\\mathcal{L}$ from a recovered batch from the previously stored results.\n",
    "\n",
    "***\n",
    "__Question 6__ Implement the class ```Memory``` that stores moves (in a replay buffer) via ```remember``` and provides a ```random_access``` to these. Specify a maximum memory size to avoid side effects. You can for example use a ```list()``` and set by default ```max_memory=100```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory(object):\n",
    "    def __init__(self, max_memory=100):\n",
    "        self.max_memory = max_memory\n",
    "        self.memory = list()\n",
    "\n",
    "    def remember(self, m):\n",
    "        self.memory.append(m)\n",
    "\n",
    "    def random_access(self): \n",
    "        return self.memory[rd.randint(0,len(self.memory)-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "The pipeline we will use for training is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent,env,epoch,prefix=''):\n",
    "    # Number of won games\n",
    "    score = 0\n",
    "    loss = 0\n",
    "\n",
    "    for e in range(epoch):\n",
    "        # At each epoch, we restart to a fresh game and get the initial state\n",
    "        state = env.reset()\n",
    "        # This assumes that the games will terminate\n",
    "        game_over = False\n",
    "\n",
    "        win = 0\n",
    "        lose = 0\n",
    "\n",
    "        while not game_over:\n",
    "            # The agent performs an action\n",
    "            action = agent.act(state)\n",
    "\n",
    "            # Apply an action to the environment, get the next state, the reward\n",
    "            # and if the games end\n",
    "            prev_state = state\n",
    "            state, reward, game_over = env.act(action)\n",
    "\n",
    "            # Update the counters\n",
    "            if reward > 0:\n",
    "                win = win + reward\n",
    "            if reward < 0:\n",
    "                lose = lose -reward\n",
    "\n",
    "            # Apply the reinforcement strategy\n",
    "            loss = agent.reinforce(prev_state, state,  action, reward, game_over)\n",
    "\n",
    "        # Save as a mp4\n",
    "        if e % 10 == 0:\n",
    "            env.draw(prefix+str(e))\n",
    "\n",
    "        # Update stats\n",
    "        score += win-lose\n",
    "\n",
    "        print(\"Epoch {:03d}/{:03d} | Loss {:.4f} | Win/lose count {}/{} ({})\"\n",
    "              .format(e, epoch, loss, win, lose, win-lose))\n",
    "        agent.save(name_weights=prefix+'model.h5',name_model=prefix+'model.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "__Question 7__ Implement the DQN training algorithm using a cascade of fully connected layers. You can use different learning rate, batch size or memory size parameters. In particular, the loss might oscillate while the player will start to win the games. You have to find a good criterium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(Agent):\n",
    "    def __init__(self, grid_size,  epsilon = 0.1, memory_size=100, batch_size = 16,n_state=2):\n",
    "        super(DQN, self).__init__(epsilon = epsilon)\n",
    "\n",
    "        # Discount for Q learning\n",
    "        self.discount = 0.99\n",
    "        \n",
    "        self.grid_size = grid_size\n",
    "        \n",
    "        # number of state\n",
    "        self.n_state = n_state\n",
    "\n",
    "        # Memory\n",
    "        self.memory = Memory(memory_size)\n",
    "        \n",
    "        # Batch size when learning\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def learned_act(self, s):\n",
    "        s = np.reshape(s,[1,5,5,self.n_state])\n",
    "        pred = self.model.predict (s) \n",
    "        return np.argmax(pred[0])\n",
    "\n",
    "    def reinforce(self, s_, n_s_, a_, r_, game_over_):\n",
    " # Two steps: first memorize the states, second learn from the pool\n",
    "\n",
    "        s_ = np.reshape(s_,[1,5,5,self.n_state])\n",
    "        n_s_ = np.reshape(n_s_,[1,5,5,self.n_state])\n",
    "        \n",
    "        self.memory.remember([s_, n_s_, a_, r_, game_over_])\n",
    "        input_states = np.zeros((self.batch_size, 5,5,self.n_state))\n",
    "    \n",
    "        \n",
    "        target_q = np.zeros((self.batch_size, 4))\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            target = r_\n",
    "            s_, n_s_,a_, r_, game_over_ = self.memory.random_access()\n",
    "            input_states[i]= s_\n",
    "            \n",
    "            if game_over_:\n",
    "                target= r_\n",
    "\n",
    "            else:               \n",
    "                target = (r_ + self.discount * np.amax(self.model.predict(n_s_)[0]))\n",
    "                \n",
    "            target_f = self.model.predict(s_)\n",
    "            target_f[0][a_] = target\n",
    "            self.model.fit(s_, target_f, epochs=1, verbose=0)\n",
    "            \n",
    "            target_q[i]=target_f\n",
    "            \n",
    "        # HINT: Clip the target to avoid exploiding gradients.. -- clipping is a bit tighter\n",
    "        target_q = np.clip(target_q, -3, 3)\n",
    "\n",
    "        l = self.model.train_on_batch(input_states, target_q)\n",
    "\n",
    "        return l\n",
    "\n",
    "    def save(self,name_weights='model.h5',name_model = 'model.json'):\n",
    "        self.model.save_weights(name_weights, overwrite=True)\n",
    "        with open(name_model, \"w\") as outfile:\n",
    "            json.dump(self.model.to_json(), outfile)\n",
    "            \n",
    "    def load(self,name_weights='model.h5',name_model='model.json'):\n",
    "        with open(name_model, \"r\") as jfile:\n",
    "            model = model_from_json(json.load(jfile))\n",
    "        model.load_weights(name_weights)\n",
    "        model.compile(\"sgd\", \"mse\")\n",
    "        self.model = model\n",
    "\n",
    "            \n",
    "class DQN_FC(DQN):\n",
    "    def __init__(self, *args, lr=0.1,**kwargs):\n",
    "        super(DQN_FC, self).__init__( *args,**kwargs)\n",
    "        \n",
    "        # NN Model\n",
    "        \n",
    "        ####### FILL IN\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Reshape((5*5*self.n_state,), input_shape=(5,5,self.n_state)))\n",
    "        model.add(Dense(128,input_shape=(50,),init='uniform',activation='relu'))\n",
    "        model.add(Dense(64,init='uniform',activation='relu'))\n",
    "        model.add(Dense(32,init='uniform',activation='relu'))\n",
    "        model.add(Dense(16,activation='relu'))\n",
    "        model.add(Dense(8,activation='relu'))\n",
    "        model.add(Dense(4,init='uniform',activation='linear'))\n",
    "        model.compile(sgd(lr=lr, decay=1e-4, momentum=0.0), \"mse\")\n",
    "        self.model = model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:83: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, input_shape=(50,), activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:84: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(64, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:85: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(32, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:88: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(4, activation=\"linear\", kernel_initializer=\"uniform\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000/020 | Loss 0.0019 | Win/lose count 2.0/5.0 (-3.0)\n",
      "Epoch 001/020 | Loss 0.1972 | Win/lose count 4.5/3.0 (1.5)\n",
      "Epoch 002/020 | Loss 0.0017 | Win/lose count 4.0/2.0 (2.0)\n",
      "Epoch 003/020 | Loss 0.0095 | Win/lose count 6.0/6.0 (0.0)\n",
      "Epoch 004/020 | Loss 0.0170 | Win/lose count 6.0/8.0 (-2.0)\n",
      "Epoch 005/020 | Loss 0.0035 | Win/lose count 5.0/4.0 (1.0)\n",
      "Epoch 006/020 | Loss 0.0696 | Win/lose count 9.0/12.0 (-3.0)\n",
      "Epoch 007/020 | Loss 0.0285 | Win/lose count 4.0/8.0 (-4.0)\n",
      "Epoch 008/020 | Loss 0.0138 | Win/lose count 2.0/6.0 (-4.0)\n",
      "Epoch 009/020 | Loss 0.0153 | Win/lose count 1.5/2.0 (-0.5)\n",
      "Epoch 010/020 | Loss 0.0026 | Win/lose count 1.5/2.0 (-0.5)\n",
      "Epoch 011/020 | Loss 0.0073 | Win/lose count 8.0/5.0 (3.0)\n",
      "Epoch 012/020 | Loss 0.0202 | Win/lose count 8.0/6.0 (2.0)\n",
      "Epoch 013/020 | Loss 0.0021 | Win/lose count 3.0/10.0 (-7.0)\n",
      "Epoch 014/020 | Loss 0.0087 | Win/lose count 6.5/7.0 (-0.5)\n",
      "Epoch 015/020 | Loss 0.0126 | Win/lose count 4.0/5.0 (-1.0)\n",
      "Epoch 016/020 | Loss 0.0034 | Win/lose count 4.5/1.0 (3.5)\n",
      "Epoch 017/020 | Loss 0.0045 | Win/lose count 5.5/9.0 (-3.5)\n",
      "Epoch 018/020 | Loss 0.0084 | Win/lose count 2.0/0 (2.0)\n",
      "Epoch 019/020 | Loss 0.0051 | Win/lose count 1.5/5.0 (-3.5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video alt=\"test\" controls>\n",
       "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
       "             </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = Environment(grid_size=size, max_time=T, temperature=0.3)\n",
    "agent = DQN_FC(size, lr=.1, epsilon = 0.1, memory_size=2000, batch_size = 32)\n",
    "train(agent, env, epochs_train, prefix='fc_train')\n",
    "HTML(display_videos('fc_train10.mp4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "__Question 8__ Implement the DQN training algorithm using a CNN (for example, 2 convolutional layers and one final fully connected layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_CNN(DQN):\n",
    "    def __init__(self, *args,lr=0.1,**kwargs):\n",
    "        super(DQN_CNN, self).__init__(*args,**kwargs)\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, (2,2), input_shape=(5,5,self.n_state), activation='relu'))\n",
    "        model.add(Conv2D(32, (2,2), activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(4, activation='linear'))\n",
    "        model.compile(sgd(lr=lr, decay=1e-4, momentum=0.0), \"mse\")\n",
    "        self.model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000/020 | Loss nan | Win/lose count 3.0/7.0 (-4.0)\n",
      "Epoch 001/020 | Loss nan | Win/lose count 4.0/1.0 (3.0)\n",
      "Epoch 002/020 | Loss nan | Win/lose count 2.0/3.0 (-1.0)\n",
      "Epoch 003/020 | Loss nan | Win/lose count 1.0/0 (1.0)\n",
      "Epoch 004/020 | Loss nan | Win/lose count 1.0/1.0 (0.0)\n",
      "Epoch 005/020 | Loss nan | Win/lose count 1.5/0 (1.5)\n",
      "Epoch 006/020 | Loss nan | Win/lose count 2.5/3.0 (-0.5)\n",
      "Epoch 007/020 | Loss nan | Win/lose count 2.0/7.0 (-5.0)\n",
      "Epoch 008/020 | Loss nan | Win/lose count 3.0/5.0 (-2.0)\n",
      "Epoch 009/020 | Loss nan | Win/lose count 1.5/2.0 (-0.5)\n",
      "Epoch 010/020 | Loss nan | Win/lose count 2.5/4.0 (-1.5)\n",
      "Epoch 011/020 | Loss nan | Win/lose count 3.5/3.0 (0.5)\n",
      "Epoch 012/020 | Loss nan | Win/lose count 2.5/2.0 (0.5)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-0f9cf5542594>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_time\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQN_CNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cnn_train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisplay_videos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cnn_train10.mp4'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-24317fb268aa>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(agent, env, epoch, prefix)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;31m# Apply the reinforcement strategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreinforce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame_over\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# Save as a mp4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-cdbbfc5e12b9>\u001b[0m in \u001b[0;36mreinforce\u001b[0;34m(self, s_, n_s_, a_, r_, game_over_)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mr_\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscount\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_s_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mtarget_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = Environment(grid_size=size, max_time=T, temperature=0.3)\n",
    "agent = DQN_CNN(size, lr=.1, epsilon = 0.1, memory_size=2000, batch_size = 32)\n",
    "train(agent,env,epochs_train,prefix='cnn_train')\n",
    "HTML(display_videos('cnn_train10.mp4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "__Question 9__ Test both algorithms and compare their performances. Which issue(s) do you observe? Observe also different behaviors by changing the temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:83: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, input_shape=(50,), activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:84: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(64, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:85: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(32, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:88: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(4, activation=\"linear\", kernel_initializer=\"uniform\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test of the CNN\n",
      "Win/lose count 2.5/2.0. Average score (0.5)\n",
      "Win/lose count 0/1.0. Average score (-0.25)\n",
      "Win/lose count 0.5/2.0. Average score (-0.6666666666666666)\n",
      "Win/lose count 0/0. Average score (-0.5)\n",
      "Win/lose count 0.5/0. Average score (-0.3)\n",
      "Win/lose count 1.5/3.0. Average score (-0.5)\n",
      "Win/lose count 0.5/1.0. Average score (-0.5)\n",
      "Win/lose count 1.0/1.0. Average score (-0.4375)\n",
      "Win/lose count 0.5/2.0. Average score (-0.5555555555555556)\n",
      "Win/lose count 0.5/1.0. Average score (-0.55)\n",
      "Win/lose count 0/1.0. Average score (-0.5909090909090909)\n",
      "Win/lose count 1.0/3.0. Average score (-0.7083333333333334)\n",
      "Win/lose count 1.0/2.0. Average score (-0.7307692307692307)\n",
      "Win/lose count 0/0. Average score (-0.6785714285714286)\n",
      "Win/lose count 0.5/0. Average score (-0.6)\n",
      "Win/lose count 1.0/3.0. Average score (-0.6875)\n",
      "Win/lose count 0.5/1.0. Average score (-0.6764705882352942)\n",
      "Win/lose count 0.5/0. Average score (-0.6111111111111112)\n",
      "Win/lose count 0/1.0. Average score (-0.631578947368421)\n",
      "Win/lose count 2.0/2.0. Average score (-0.6)\n",
      "Final score: -0.6\n",
      "Test of the FC\n",
      "Win/lose count 0.5/0. Average score (0.5)\n",
      "Win/lose count 0.5/0. Average score (0.5)\n",
      "Win/lose count 0/2.0. Average score (-0.3333333333333333)\n",
      "Win/lose count 1.0/3.0. Average score (-0.75)\n",
      "Win/lose count 1.5/1.0. Average score (-0.5)\n",
      "Win/lose count 0.5/0. Average score (-0.3333333333333333)\n",
      "Win/lose count 0/0. Average score (-0.2857142857142857)\n",
      "Win/lose count 1.0/1.0. Average score (-0.25)\n",
      "Win/lose count 0/1.0. Average score (-0.3333333333333333)\n",
      "Win/lose count 0.5/0. Average score (-0.25)\n",
      "Win/lose count 1.0/1.0. Average score (-0.22727272727272727)\n",
      "Win/lose count 1.0/1.0. Average score (-0.20833333333333334)\n",
      "Win/lose count 0/2.0. Average score (-0.34615384615384615)\n",
      "Win/lose count 1.0/3.0. Average score (-0.4642857142857143)\n",
      "Win/lose count 3.0/5.0. Average score (-0.5666666666666667)\n",
      "Win/lose count 0/2.0. Average score (-0.65625)\n",
      "Win/lose count 0/0. Average score (-0.6176470588235294)\n",
      "Win/lose count 1.5/2.0. Average score (-0.6111111111111112)\n",
      "Win/lose count 1.0/4.0. Average score (-0.7368421052631579)\n",
      "Win/lose count 1.0/0. Average score (-0.65)\n",
      "Final score: -0.65\n"
     ]
    }
   ],
   "source": [
    "env = Environment(grid_size=size, max_time=T,temperature=0.3)\n",
    "agent_cnn = DQN_CNN(size, lr=.1, epsilon = 0.1, memory_size=2000, batch_size = 32)\n",
    "agent_cnn.load(name_weights='cnn_trainmodel.h5',name_model='cnn_trainmodel.json')\n",
    "\n",
    "agent_fc = DQN_FC(size, lr=.1, epsilon = 0.1, memory_size=2000, batch_size = 32)\n",
    "agent_cnn.load(name_weights='fc_trainmodel.h5',name_model='fc_trainmodel.json')\n",
    "print('Test of the CNN')\n",
    "test(agent_cnn,env,epochs_test,prefix='cnn_test')\n",
    "print('Test of the FC')\n",
    "test(agent_fc,env,epochs_test,prefix='fc_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video alt=\"test\" controls>\n",
       "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
       "             </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(display_videos('cnn_test10.mp4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video alt=\"test\" controls>\n",
       "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
       "             </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(display_videos('fc_test10.mp4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First we observe that the model'sexploration rate is low. In fact, only the current board is explored. Another observation is the performance of both CNN and FC models, where we notice that the CNN model outperforms the FC model.\n",
    "\n",
    "- Changing the temperature results in better scores. However, this approach is biased and cannot be considered as a good solution since it is playing with the binomial law."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "The algorithm tends to not explore the map which can be an issue. We propose two ideas in order to encourage exploration:\n",
    "1. Incorporating a decreasing $\\epsilon$-greedy exploration. You can use the method ```set_epsilon```\n",
    "2. Append via the environment a new state that describes if a cell has been visited or not\n",
    "\n",
    "***\n",
    "__Question 10__ Design a new ```train_explore``` function and environment class ```EnvironmentExploring``` to tackle the issue of exploration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_explore(agent,env,epoch,prefix=''):\n",
    "    # Number of won games\n",
    "    score = 0\n",
    "    loss = 0\n",
    "\n",
    "    for e in range(epoch):\n",
    "        # At each epoch, we restart to a fresh game and get the initial state\n",
    "        state = env.reset()\n",
    "        # This assumes that the games will terminate\n",
    "        game_over = False\n",
    "\n",
    "        win = 0\n",
    "        lose = 0\n",
    "\n",
    "        while not game_over:\n",
    "            \n",
    "            # Epsilon-greedy \n",
    "            agent.set_epsilon(np.float(agent.get_epsilon())*0.995)\n",
    "            \n",
    "            # The agent performs an action\n",
    "            action = agent.act(state)\n",
    "\n",
    "            # Apply an action to the environment, get the next state, the reward\n",
    "            # and if the games end\n",
    "            prev_state = state\n",
    "            state, reward, game_over = env.act(action, train=True)\n",
    "\n",
    "            # Update the counters\n",
    "            if reward > 0:\n",
    "                win = win + reward\n",
    "            if reward < 0:\n",
    "                lose = lose - reward\n",
    "\n",
    "            # Apply the reinforcement strategy\n",
    "            loss = agent.reinforce(prev_state, state,  action, reward, game_over)\n",
    "            \n",
    "        # Save as a mp4       \n",
    "        env.draw(prefix+str(e))\n",
    "\n",
    "        # Update stats\n",
    "        score += win-lose\n",
    "\n",
    "        print(\"Epoch {:03d}/{:03d} | Loss {:.4f} | Win/lose count {}/{} ({})\"\n",
    "              .format(e, epoch, loss, win, lose, win-lose))\n",
    "        agent.save(name_weights=prefix+'model.h5',name_model=prefix+'model.json')\n",
    "        \n",
    "class EnvironmentExploring(object):\n",
    "    def __init__(self, grid_size=10, max_time=500, temperature=0.1):\n",
    "        grid_size = grid_size + 4\n",
    "        self.grid_size = grid_size\n",
    "        self.max_time = max_time\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        self.board = np.zeros([grid_size,grid_size])\n",
    "        self.position = np.zeros([grid_size,grid_size])\n",
    "        self.malus_position = np.zeros([grid_size,grid_size])\n",
    "        \n",
    "        # coordinate of the rat\n",
    "        self.x = 0\n",
    "        self.y = 1\n",
    "        \n",
    "        # self time\n",
    "        self.t = 0\n",
    "\n",
    "        self.scale=16\n",
    "\n",
    "        self.to_draw = np.zeros((max_time+2, grid_size*self.scale, grid_size*self.scale, 3))\n",
    "        \n",
    "    def get_t(self):\n",
    "        return self.t\n",
    "    \n",
    "    def draw(self,e):\n",
    "        skvideo.io.vwrite(str(e) + '.mp4', self.to_draw)\n",
    "\n",
    "    def get_frame(self,t):\n",
    "        b = np.zeros((self.grid_size,self.grid_size,3))+128\n",
    "        b[self.board>0,0] = 256\n",
    "        b[self.board < 0, 2] = 256\n",
    "        b[self.x,self.y,:]=256\n",
    "        b[-2:,:,:]=0\n",
    "        b[:,-2:,:]=0\n",
    "        b[:2,:,:]=0\n",
    "        b[:,:2,:]=0\n",
    "        \n",
    "        b =  cv2.resize(b, None, fx=self.scale, fy=self.scale, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        self.to_draw[t,:,:,:]=b\n",
    "             \n",
    "        \n",
    "    def act(self, action, train=True):\n",
    "        \"\"\"This function returns the new state, reward and decides if the\n",
    "        game ends.\"\"\"\n",
    "\n",
    "        self.get_frame(int(self.t))\n",
    "\n",
    "        self.position = np.zeros((self.grid_size, self.grid_size))\n",
    "\n",
    "        self.position[0:2,:]= -1\n",
    "        self.position[:,0:2] = -1\n",
    "        self.position[-2:, :] = -1\n",
    "        self.position[:, -2:] = -1\n",
    "\n",
    "        self.position[self.x, self.y] = 1\n",
    "        \n",
    "        if action == 0:\n",
    "            if self.x == self.grid_size-3:\n",
    "                self.x = self.x-1\n",
    "            else:\n",
    "                self.x = self.x + 1\n",
    "        elif action == 1:\n",
    "            if self.x == 2:\n",
    "                self.x = self.x+1\n",
    "            else:\n",
    "                self.x = self.x-1\n",
    "        elif action == 2:\n",
    "            if self.y == self.grid_size - 3:\n",
    "                self.y = self.y - 1\n",
    "            else:\n",
    "                self.y = self.y + 1\n",
    "        elif action == 3:\n",
    "            if self.y == 2:\n",
    "                self.y = self.y + 1\n",
    "            else:\n",
    "                self.y = self.y - 1\n",
    "        else:\n",
    "            RuntimeError('Error: action not recognized')\n",
    "        \n",
    "        self.t = self.t + 1\n",
    "        \n",
    "        ## In Environment exploring:\n",
    "        # You will have to change n_state to 3 because you will use one more layer!\n",
    "        reward = 0\n",
    "        if train:\n",
    "            reward -= self.malus_position[self.x, self.y]\n",
    "\n",
    "        self.malus_position[self.x, self.y] = 0.01\n",
    "        reward = reward + self.board[self.x, self.y]\n",
    "        self.board[self.x, self.y] = 0\n",
    "        game_over = self.t > self.max_time\n",
    "\n",
    "        # 3 \"feature\" states instead of 2\n",
    "        state = np.concatenate((self.malus_position.reshape(self.grid_size, self.grid_size,1),\n",
    "                                self.board.reshape(self.grid_size, self.grid_size,1),\n",
    "                                self.position.reshape(self.grid_size, self.grid_size,1)),axis=2)\n",
    "        \n",
    "        state = state[self.x-2:self.x+3,self.y-2:self.y+3,:]\n",
    "        \n",
    "        return state, reward, game_over\n",
    "    def reset(self):\n",
    "        \"\"\"This function resets the game and returns the initial state\"\"\"\n",
    "\n",
    "        self.x = np.random.randint(3, self.grid_size-3, size=1)[0]\n",
    "        self.y = np.random.randint(3, self.grid_size-3, size=1)[0]\n",
    "\n",
    "\n",
    "        bonus = 0.5*np.random.binomial(1,self.temperature,size=self.grid_size**2)\n",
    "        bonus = bonus.reshape(self.grid_size,self.grid_size)\n",
    "\n",
    "        malus = -1.0*np.random.binomial(1,self.temperature,size=self.grid_size**2)\n",
    "        malus = malus.reshape(self.grid_size, self.grid_size)\n",
    "\n",
    "        self.to_draw = np.zeros((self.max_time+2, self.grid_size*self.scale, self.grid_size*self.scale, 3))\n",
    "\n",
    "\n",
    "        malus[bonus>0]=0\n",
    "\n",
    "        self.board = bonus + malus\n",
    "\n",
    "        self.position = np.zeros((self.grid_size, self.grid_size))\n",
    "        self.position[0:2,:]= -1\n",
    "        self.position[:,0:2] = -1\n",
    "        self.position[-2:, :] = -1\n",
    "        self.position[:, -2:] = -1\n",
    "        self.board[self.x,self.y] = 0\n",
    "       \n",
    "        self.malus_position[self.x, self.y] = 0\n",
    "\n",
    "        self.t = 0\n",
    "\n",
    "        state = np.concatenate((self.malus_position.reshape(self.grid_size, self.grid_size,1),\n",
    "                                self.board.reshape(self.grid_size, self.grid_size,1),\n",
    "                                self.position.reshape(self.grid_size, self.grid_size,1)),axis=2)\n",
    "\n",
    "        state = state[self.x - 2:self.x + 3, self.y - 2:self.y + 3, :]\n",
    "        \n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000/020 | Loss 0.0019 | Win/lose count 3.5/2.7799999999999843 (0.7200000000000157)\n",
      "Epoch 001/020 | Loss 0.0059 | Win/lose count 2.95/5.909999999999959 (-2.959999999999959)\n",
      "Epoch 002/020 | Loss 0.0032 | Win/lose count 2.96/3.9399999999999595 (-0.9799999999999596)\n",
      "Epoch 003/020 | Loss 0.0009 | Win/lose count 2.48/2.939999999999981 (-0.45999999999998087)\n",
      "Epoch 004/020 | Loss 0.0222 | Win/lose count 10.930000000000001/2.6399999999999872 (8.290000000000013)\n",
      "Epoch 005/020 | Loss 0.0238 | Win/lose count 10.920000000000002/3.6799999999999864 (7.240000000000015)\n",
      "Epoch 006/020 | Loss 0.2151 | Win/lose count 6.870000000000002/1.8400000000000014 (5.03)\n",
      "Epoch 007/020 | Loss 0.1410 | Win/lose count 10.320000000000002/1.7900000000000014 (8.530000000000001)\n",
      "Epoch 008/020 | Loss 0.0485 | Win/lose count 2.9400000000000004/2.949999999999988 (-0.009999999999987796)\n",
      "Epoch 009/020 | Loss 0.1703 | Win/lose count 14.760000000000003/2.6799999999999926 (12.08000000000001)\n",
      "Epoch 010/020 | Loss 0.1014 | Win/lose count 19.74/6.539999999999976 (13.200000000000022)\n",
      "Epoch 011/020 | Loss 0.1311 | Win/lose count 8.840000000000002/3.829999999999965 (5.010000000000037)\n",
      "Epoch 012/020 | Loss 0.2486 | Win/lose count 8.350000000000001/2.8099999999999996 (5.540000000000002)\n",
      "Epoch 013/020 | Loss 0.0826 | Win/lose count 5.3900000000000015/1.8900000000000015 (3.5)\n",
      "Epoch 014/020 | Loss 0.1477 | Win/lose count 15.700000000000005/3.6699999999999866 (12.030000000000019)\n",
      "Epoch 015/020 | Loss 0.2789 | Win/lose count 17.72/2.6199999999999872 (15.100000000000012)\n",
      "Epoch 016/020 | Loss 0.0982 | Win/lose count 8.340000000000002/5.809999999999964 (2.5300000000000376)\n",
      "Epoch 017/020 | Loss 0.0685 | Win/lose count 2.45/2.9599999999999804 (-0.5099999999999802)\n",
      "Epoch 018/020 | Loss 0.0347 | Win/lose count 20.14999999999999/6.569999999999969 (13.580000000000023)\n",
      "Epoch 019/020 | Loss 0.3173 | Win/lose count 4.420000000000001/1.9000000000000015 (2.5199999999999996)\n",
      "Final score: 5.4990000000000165\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video alt=\"test\" controls>\n",
       "                <source src=\"data:video/mp4;base64,AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1wNDEAAAAIZnJlZQAAGB5tZGF0AAACnwYF//+b3EXpvebZSLeWLNgg2SPu73gyNjQgLSBjb3JlIDE1MiAtIEguMjY0L01QRUctNCBBVkMgY29kZWMgLSBDb3B5bGVmdCAyMDAzLTIwMTcgLSBodHRwOi8vd3d3LnZpZGVvbGFuLm9yZy94MjY0Lmh0bWwgLSBvcHRpb25zOiBjYWJhYz0xIHJlZj0zIGRlYmxvY2s9MTowOjAgYW5hbHlzZT0weDE6MHgxMTEgbWU9aGV4IHN1Ym1lPTcgcHN5PTEgcHN5X3JkPTEuMDA6MC4wMCBtaXhlZF9yZWY9MSBtZV9yYW5nZT0xNiBjaHJvbWFfbWU9MSB0cmVsbGlzPTEgOHg4ZGN0PTAgY3FtPTAgZGVhZHpvbmU9MjEsMTEgZmFzdF9wc2tpcD0xIGNocm9tYV9xcF9vZmZzZXQ9NCB0aHJlYWRzPTYgbG9va2FoZWFkX3RocmVhZHM9MSBzbGljZWRfdGhyZWFkcz0wIG5yPTAgZGVjaW1hdGU9MSBpbnRlcmxhY2VkPTAgYmx1cmF5X2NvbXBhdD0wIGNvbnN0cmFpbmVkX2ludHJhPTAgYmZyYW1lcz0zIGJfcHlyYW1pZD0yIGJfYWRhcHQ9MSBiX2JpYXM9MCBkaXJlY3Q9MSB3ZWlnaHRiPTEgb3Blbl9nb3A9MCB3ZWlnaHRwPTIga2V5aW50PTI1MCBrZXlpbnRfbWluPTI1IHNjZW5lY3V0PTQwIGludHJhX3JlZnJlc2g9MCByY19sb29rYWhlYWQ9NDAgcmM9Y3JmIG1idHJlZT0xIGNyZj0yMy4wIHFjb21wPTAuNjAgcXBtaW49MCBxcG1heD02OSBxcHN0ZXA9NCBpcF9yYXRpbz0xLjQwIGFxPTE6MS4wMACAAAAC5WWIhAAz//72hvgU2FMj/k+//CP/uR+3H1n64sZCCb61XkPRgDJfGv+ZRmGl6cnPLe0fr0zQBGc/GUvKWP7KPMsV4ZVm0PgU1dJ1fgUlChqkGba6LmNWXtq8n/zYfc15Bm2GjnRvXxqrlaHKS06B+iDlulYhtyr4NwHHY7RKsqc7ZjVZB5BSJIVvIkK1xfwD2D455wZmp9Cs8fJntPjFhIPQAAAECxOIa/jf1kn5/0fTOA9qhKXgWhs/65wlXKNTzX70VReNwQlXiOp5q2zLYQpBpoWcPP+KcPWs1tdpiVp29Hy/YgOorrQMUZqZ5dmdyAqGJwU4nKJJ8E0zW5CL7o0OoB+rXT/Tq6uBZ82EM3BfjUee57wNx+wRFAC3JK0uJP5EUwGA0DRnMYOgMUkw3NzAGp0JyjtpMjJPuZg+HhDXVMaVuAHXcWb439CttOGsRV5fMZYtbyg5SCJ2cDmmxqLfqWZ+utU4cSN8djrwPk3u5hmdT3IUsADQYAdCydOqECYP8WJC4OXJlqWmOovBsTwcuS+G10p3BHnOmHHCkMXxpaqHn3yfYYe6eaMK7aFm6cahP51huTmrXA4OrHXDBHLZw0kvkFkidKIPcO/th+m/XaJ0BEUX7WUcsB2QDp9U3B9x3G/CgGLspbuYYn5wNQ/5VPRL0JJQ7X/a7um2ux9xo1jrpJYO9zBkN5imFKWWQQq6Scp7TpA1G2piY6sIO2GOzAEX2BXEqwmugLBqBOtODn28fj9MngFcGVPKnQaW0YxLKdGR18laoUy7akpXxpTzhHflnd0qWOU1wm/sgyJFkzB1pU3sWAhO/WPaxH7tqCnty21sGBCXv9yvHtcdUrbucE/XEyXIsP53N3BzzNrEn5+PBkWbuRLpAAw1a44Bi1AoEfDtIi39R7DRim3nehIaA0+qmCl1BKEsvoMjuUgmRfwAbq8ouPe5azzPtrFnTV8tMfP2OtFyyyUkttey/P5JLgAFPQAAABNBmiFsQ3/+p4QABdfjTpArMpyRAAAAGkGaQjwhkymEN//+p4QABbPj9BDOgrWZTk2BAAAAHUGaZUnhDyZTAhv//qeEAAhqALNnwf544PKv1QTAAAAAEkGeg0URPCv/AAbp1vKNXhJW5wAAABABnqRqQr8ABunVPJgevmuBAAAAHkGap0moQWiZTBTw3/6nhAANe6tUx/qSfOWZHPwT1QAAABABnsZqQr8ACxWPLcNm1Z+BAAAAGUGayEnhClJlMCG//qeEAA3Lq0ghE/y3n4AAAAAfQZrqSeEOiZTBTRMO//6plgAG+9pf2LAdEC3E2lzjngAAABABnwlqQr8AC1tyGH0BIO0pAAAAGUGbDknhDyZTAhv//qeEAA43vs+7Wrb0NbAAAAAQQZ8sRRE8L/8ACG5+5bpjkAAAABABn0t0Qr8AB5mwNb6WNpe5AAAAEAGfTWpCvwALpG13WQw5jUEAAAAaQZtRSahBaJlMCG///qeEAAlqXtIIRP8uA0EAAAAPQZ9vRREsK/8AB5gVw5nAAAAADQGfkGpCvwAHmr8YVM4AAAATQZuTSahBbJlMFEw3//6nhAABJwAAAA8Bn7JqQr8AB4VDdhnq0IkAAAASQZu1SeEKUmUwUsN//qeEAAEnAAAADwGf1GpCvwAHhUN2GerQiQAAABJBm9dJ4Q6JlMFEw3/+p4QAAScAAAAPAZ/2akK/AAeFQ3YZ6tCJAAAAEkGb+UnhDyZTBTw3//6nhAABJwAAAA8BnhhqQr8AB4VDdhnq0IkAAAASQZobSeEPJlMFPDf//qeEAAEnAAAADwGeOmpCvwAHhUN2GerQiQAAABxBmj1J4Q8mUwU8O//+qZYABMfjz+XZ7ULIUukrAAAAEAGeXGpCvwAHmBec60MMCcEAAAARQZpBSeEPJlMCG//+p4QAAScAAAAMQZ5/RRE8L/8AALKAAAAAEAGennRCvwAEyVI78AH3UMEAAAAQAZ6AakK/AATJUjvZ4+6hgAAAABJBmoVJqEFomUwIZ//+nhAABH0AAAAMQZ6jRREsL/8AALKAAAAAEAGewnRCvwAEyVI78AH3UMEAAAAQAZ7EakK/AATJUjvZ4+6hgQAAABtBmsZJqEFsmUwIb//+p4QABh3VpBCUQ4P5cW8AAAAbQZrnSeEKUmUwIb/+p4QABkXVpBCUQJ3/RxxBAAAAE0GbCUnhDomUwU0TDf/+p4QAAScAAAAPAZ8oakK/AAUelG9VgDfgAAAAEkGbK0nhDyZTBTw3//6nhAABJwAAABABn0pqQr8ABR1G6yoVhJ6gAAAAG0GbT0nhDyZTAhv//qeEAAZF1eQmZUUH4oTegwAAABBBn21FETwv/wADtfw9ddZBAAAAEAGfjHRCvwAFHTWjJLf7VMEAAAAPAZ+OakK/AAT6Nru+77rBAAAAGUGbkkmoQWiZTAhn//6eEAAPP6+7tObuMS4AAAASQZ+wRREsK/8ABPsHXd39IyrAAAAAEAGf0WpCvwAE+a+c60MMMMEAAAAZQZvTSahBbJlMCG///qeEAAPP7B69mfBGhwAAABhBm/RJ4QpSZTAhv/6nhAADuewevZnwRo8AAAAYQZoXSeEOiZTAhv/+p4QAA6PsHr2Z8EaXAAAAEkGeNUURPCv/AASXp13d/SM1gAAAAA4BnlZqQr8ABJZXXceFrQAAABlBmlhJqEFomUwIb//+p4QAA43sHr2Z8EafAAAAGUGae0nhClJlMCG//qeEAAVj0T/Vb5j8jcAAAAAPQZ6ZRTRMK/8ABFZXAqfBAAAADwGeumpCvwAEdeaJqSqTgAAAABpBmrxJqEFomUwIb//+p4QACCoAs22z7Pn2QQAAABtBmt1J4QpSZTAh3/6plgAGeqQZoA9SQY/8X+EAAAAeQZrhSeEOiZTAhv/+p4QAFGxWqY/0cT/IUM5V+pi0AAAAEUGfH0URPC//AAxCpoWAcKioAAAADwGfPnRCvwAKzaO884wpgQAAAA8BnyBqQr8AEF2I8mB69/cAAAAdQZslSahBaJlMCGf//p4QAHn9fd2nSNgvQfR5bKkAAAAQQZ9DRREsL/8AEtzxlwYXWAAAABABn2J0Qr8AGSk0InxZikPxAAAADwGfZGpCvwAZwFjYHKdggQAAABlBm2ZJqEFsmUwIZ//+nhAAdz193ac3cXCnAAAAGUGbh0nhClJlMCG//qeEAB3PfZj/D6tuUIEAAAAZQZuoSeEOiZTAhv/+p4QALV6J/qt8x+JJwAAAAB9Bm8xJ4Q8mUwIb//6nhABDR81TWbdR6Oebbc8teySmAAAAEkGf6kURPC//ACj0CT4sTGvdIQAAAA4Bngl0Qr8AJbuO884uBwAAABABngtqQr8AN07cJuM+vTtMAAAAGUGaDUmoQWiZTAhv//6nhABDvjpj/D6tt3EAAAAcQZoxSeEKUmUwIZ/+nhABk/X3dp0jYL0H0eWPSQAAABBBnk9FNEwv/wA8yat0R9vdAAAAEAGebnRCvwBR80SJ8WYo5BAAAAAQAZ5wakK/AFQUaJl0HTy8WAAAABlBmnJJqEFomUwIZ//+nhACWnCOfw5zfWX/AAAAGEGak0nhClJlMCGf/p4QAl3xDzrdAyQ1DAAAABlBmrRJ4Q6JlMCG//6nhADmnGf6rfMfiDjgAAAAGUGa1UnhDyZTAhv//qeEAXIYY1QHGn6xbMEAAAAeQZr4SeEPJlMCG//+p4QDkoZ/qPfS6BCf1KPj7T/AAAAAEkGfFkURPCv/AcZnzLeG5BxIuQAAAA8BnzdqQr8BxmfMb+BpOOEAAAAcQZs6SahBaJlMFPDf/qeEBC43PZIMfZ0OvgEccAAAABABn1lqQr8B3x4PJcz4VJeBAAAAGEGbW0nhClJlMCG//qeEBJIzHkgx+ZHikgAAAB9Bm31J4Q6JlMFNEw3//qeEBQhbGBDMfjqABYh2PE7BAAAAEAGfnGpCvwH5mjeaYU2dWcEAAAAoQZuBSeEPJlMCGf/+nhAGh7pvrbfgChcCmyfZ/gUzXEHwKJN3v89fugAAABZBn79FETwv/wDs/xVT/osYSlfNxHSQAAAAEAGf3nRCvwFITqTyvyU2VBEAAAAQAZ/AakK/ANeTJNN9JBxScAAAABlBm8JJqEFomUwIZ//+nhACwV7jQum+620FAAAAGEGb40nhClJlMCGf/p4QAtNe40Lpvuts3AAAABhBmgRJ4Q6JlMCG//6nhAC94rSCET/LbQMAAAAZQZolSeEPJlMCG//+p4QAwrq0dH3GzBXWUQAAABtBmkhJ4Q8mUwIZ//6eEAMP6+/VCOrjGPqiCcEAAAARQZ5mRRE8K/8Ao9KN5pveoV8AAAAOAZ6HakK/AKO2MeiK2+YAAAAaQZqJSahBaJlMCG///qeEAHy9g/wnBboSW0AAAAAeQZqrSeEKUmUwURLDf/6nhABUfjT8dt4iZmpt0hkvAAAAEAGeympCvwBDc0bzTFW0s0AAAAAZQZrMSeEOiZTAhv/+p4QANy6tIIRP8tvfgAAAABtBmvBJ4Q8mUwIb//6nhAA3fsH+WukPUrdEsh0AAAATQZ8ORRE8L/8AILP91mJ9nTPGJwAAABABny10Qr8ALWnMcB+T//NhAAAAEAGfL2pCvwAc/nDXvNKzpcAAAAAaQZsxSahBaJlMCG///qeEACLfHT6jjQkOYEAAAAAdQZtTSeEKUmUwURLDv/6plgALf76vvRNTqEG4PG8AAAAPAZ9yakK/ABJZW6UaQ8aWAAAAHEGbdUnhDomUwUTDv/6plgAHJ+FH3omp1CDcHt0AAAAPAZ+UakK/AAujbdKNIePrAAAAEkGbmUnhDyZTAh3//qmWAACVgAAAABNBn7dFETwv/wAFZyWzUzLLkNTNAAAAEAGf1nRCvwAHP4nik2yWI4EAAAAQAZ/YakK/AAdBmDyYHr5igAAAABNBm91JqEFomUwId//+qZYAAJWBAAAAEEGf+0URLC//AAVnJbN+kfoAAAAQAZ4adEK/AAc/ieKTbJYjgQAAABABnhxqQr8AB0GYPJgevmKBAAAAGUGaAUmoQWyZTAh3//6plgAEh+PP5dozc6kAAAAQQZ4/RRUsL/8ABWWWCfH8wAAAABABnl50Qr8ABz+J4pNsliOBAAAADwGeQGpCvwAHFNQ6Fo37QAAAABNBmkVJqEFsmUwId//+qZYAAJWBAAAAE0GeY0UVLC//AAUdNn5m3E5Ivw4AAAAQAZ6CdEK/AAbqTQifFmKbmQAAABABnoRqQr8ABxVcGuPFW3ehAAAAE0GaiUmoQWyZTAh3//6plgAAlYEAAAAQQZ6nRRUsL/8ABR8lufrjlwAAABABnsZ0Qr8ABupNCJ8WYpuYAAAAEAGeyGpCvwAHFVwa48Vbd6AAAAATQZrNSahBbJlMCHf//qmWAACVgQAAABBBnutFFSwv/wAFHyW5+uOXAAAAEAGfCnRCvwAG6k0InxZim5gAAAAQAZ8MakK/AAcVXBrjxVt3oQAAABxBmxFJqEFsmUwId//+qZYAAt/vq+9E1OoQbhC/AAAAEEGfL0UVLC//AANgI3e4X0EAAAAQAZ9OdEK/AASV1aMkt/tjgAAAAA8Bn1BqQr8ABHbWu77vwsAAAAATQZtVSahBbJlMCHf//qmWAACVgQAAABBBn3NFFSwv/wADYORHu7t9AAAAEAGfknRCvwAEl81QOnaikYAAAAAQAZ+UakK/AASWT5zrQww4wQAAABNBm5lJqEFsmUwId//+qZYAAJWAAAAADEGft0UVLC//AACygQAAABABn9Z0Qr8ABHhAHP60DpLBAAAAEAGf2GpCvwAEdta7rIYdJYAAAAATQZvdSahBbJlMCHf//qmWAACVgQAAABRBn/tFFSwv/wADX+uPb24aetdiyAAAABABnhp0Qr8ABJfNUDp2opGBAAAAEAGeHGpCvwAElk+c60MMOMEAAAASQZoBSahBbJlMCG///qeEAAEnAAAAEEGeP0UVLC//AANhEtm/SbYAAAAQAZ5edEK/AASXzVA6dqKRgQAAABABnkBqQr8ABJZPnOtDDDjAAAAAEkGaRUmoQWyZTAhv//6nhAABJwAAABBBnmNFFSwv/wADYRLZv0m2AAAAEAGegnRCvwAEl81QOnaikYEAAAAQAZ6EakK/AASWT5zrQww4wQAAABpBmodJqEFsmUwUTDf//qeEAAWsFB3b7B+xkwAAAA8BnqZqQr8ABJdiPJgevscAAAAYQZqoSeEKUmUwIb/+p4QABc8VpBCJ/lxrAAAAGUGayUnhDomUwId//qmWAAMBBZXGaX9sSMAAAAAnQZrtSeEPJlMCHf/+qZYAAy3sN8yyxhVXgUzXFTwKJPn2vMDh/WMnAAAAEEGfC0URPC//AAO1/6uQGUAAAAAPAZ8qdEK/AAUfLBg2Y4rhAAAADwGfLGpCvwAFH5QPJgl6gQAAABtBmzFJqEFomUwId//+qZYAAymOQf2yjlBuxIcAAAAQQZ9PRREsL/8AA7X8PXXWQQAAABABn250Qr8ABR01oyS3+1TAAAAADwGfcGpCvwAE+ja7vu+6wAAAABNBm3VJqEFsmUwId//+qZYAAJWBAAAADEGfk0UVLC//AACygAAAABABn7J0Qr8ABPugHQsYnHeQAAAAEAGftGpCvwAE+ja7vJKTTyEAAAATQZu5SahBbJlMCHf//qmWAACVgAAAAAxBn9dFFSwv/wAAsoEAAAAQAZ/2dEK/AAT7oB0LGJx3kQAAABABn/hqQr8ABPo2u7ySk08gAAAAIEGb/UmoQWyZTAh3//6plgADLKggLRQiS/32tAP77yGBAAAAFUGeG0UVLC//AAO2nrGB+ixbBcpS0AAAABABnjp0Qr8ABNfUSJ8WYqIxAAAAEAGePGpCvwAFHsI8lzPlcIEAAAAYQZohSahBbJlMCG///qeEAAZG1B3W3tk+AAAAEEGeX0UVLC//AAO1/D111kAAAAAQAZ5+dEK/AAUdNaMkt/tUwQAAAA8BnmBqQr8ABPo2u77vusAAAAAbQZplSahBbJlMCGf//p4QABh7fvbX2PgQn62/AAAAEEGeg0UVLC//AAO2nTv87CgAAAAPAZ6idEK/AAT7oB0JybrBAAAAEAGepGpCvwAFHsI8mB6+qYEAAAAaQZqpS6hCEFskRggoB/IB/YeAIV/+OEAAEXEAAAAlQZ7HRRUsL/8CAdzqS9szCrmA6Bq1qFwJQBlok8LfMqLPIdutjQAAABABnuZ0Qr8ABR01oyS3+1TAAAAAJAGe6GpCvwKvY+1BxN2qw0km5aqGByy1u80qIJosLiLSPChf4AAAC+Btb292AAAAbG12aGQAAAAAAAAAAAAAAAAAAAPoAAAfkAABAAABAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAAALCnRyYWsAAABcdGtoZAAAAAMAAAAAAAAAAAAAAAEAAAAAAAAfkAAAAAAAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAEAAAAAAAAAAAAAAAAAAEAAAAABEAAAARAAAAAAACRlZHRzAAAAHGVsc3QAAAAAAAAAAQAAH5AAAAQAAAEAAAAACoJtZGlhAAAAIG1kaGQAAAAAAAAAAAAAAAAAADIAAAGUAFXEAAAAAAAtaGRscgAAAAAAAAAAdmlkZQAAAAAAAAAAAAAAAFZpZGVvSGFuZGxlcgAAAAotbWluZgAAABR2bWhkAAAAAQAAAAAAAAAAAAAAJGRpbmYAAAAcZHJlZgAAAAAAAAABAAAADHVybCAAAAABAAAJ7XN0YmwAAACVc3RzZAAAAAAAAAABAAAAhWF2YzEAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAABEAEQAEgAAABIAAAAAAAAAAEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAY//8AAAAvYXZjQwH0AA3/4QAXZ/QADZGbKCIR0IAAAAMAgAAAGQeKFMsBAAVo6+PESAAAABhzdHRzAAAAAAAAAAEAAADKAAACAAAAABRzdHNzAAAAAAAAAAEAAAABAAAFuGN0dHMAAAAAAAAAtQAAAAMAAAQAAAAAAQAACAAAAAACAAACAAAAAAEAAAYAAAAAAQAAAgAAAAABAAAEAAAAAAEAAAYAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAgAAAAAAgAAAgAAAAABAAAGAAAAAAEAAAIAAAAAAQAABgAAAAABAAACAAAAAAEAAAYAAAAAAQAAAgAAAAABAAAGAAAAAAEAAAIAAAAAAQAABgAAAAABAAACAAAAAAEAAAYAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAgAABAAAAAABAAAGAAAAAAEAAAIAAAAAAQAABgAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACAAAAAACAAACAAAAAAIAAAQAAAAAAQAACAAAAAACAAACAAAAAAEAAAQAAAAAAQAACAAAAAACAAACAAAAAAIAAAQAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAMAAAQAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAEAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAABAAABAAAAAABAAAIAAAAAAIAAAIAAAAAAQAABgAAAAABAAACAAAAAAEAAAQAAAAAAQAABgAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAABAAABAAAAAABAAAIAAAAAAIAAAIAAAAAAQAABAAAAAABAAAGAAAAAAEAAAIAAAAAAQAABAAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAQAAAAAAQAABgAAAAABAAACAAAAAAEAAAYAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAGAAAAAAEAAAIAAAAAAgAABAAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAAQAACgAAAAABAAAEAAAAAAEAAAAAAAAAAQAAAgAAAAABAAAKAAAAAAEAAAQAAAAAAQAAAAAAAAABAAACAAAAAAEAAAoAAAAAAQAABAAAAAABAAAAAAAAAAEAAAIAAAAAHHN0c2MAAAAAAAAAAQAAAAEAAADKAAAAAQAAAzxzdHN6AAAAAAAAAAAAAADKAAAFjAAAABcAAAAeAAAAIQAAABYAAAAUAAAAIgAAABQAAAAdAAAAIwAAABQAAAAdAAAAFAAAABQAAAAUAAAAHgAAABMAAAARAAAAFwAAABMAAAAWAAAAEwAAABYAAAATAAAAFgAAABMAAAAWAAAAEwAAACAAAAAUAAAAFQAAABAAAAAUAAAAFAAAABYAAAAQAAAAFAAAABQAAAAfAAAAHwAAABcAAAATAAAAFgAAABQAAAAfAAAAFAAAABQAAAATAAAAHQAAABYAAAAUAAAAHQAAABwAAAAcAAAAFgAAABIAAAAdAAAAHQAAABMAAAATAAAAHgAAAB8AAAAiAAAAFQAAABMAAAATAAAAIQAAABQAAAAUAAAAEwAAAB0AAAAdAAAAHQAAACMAAAAWAAAAEgAAABQAAAAdAAAAIAAAABQAAAAUAAAAFAAAAB0AAAAcAAAAHQAAAB0AAAAiAAAAFgAAABMAAAAgAAAAFAAAABwAAAAjAAAAFAAAACwAAAAaAAAAFAAAABQAAAAdAAAAHAAAABwAAAAdAAAAHwAAABUAAAASAAAAHgAAACIAAAAUAAAAHQAAAB8AAAAXAAAAFAAAABQAAAAeAAAAIQAAABMAAAAgAAAAEwAAABYAAAAXAAAAFAAAABQAAAAXAAAAFAAAABQAAAAUAAAAHQAAABQAAAAUAAAAEwAAABcAAAAXAAAAFAAAABQAAAAXAAAAFAAAABQAAAAUAAAAFwAAABQAAAAUAAAAFAAAACAAAAAUAAAAFAAAABMAAAAXAAAAFAAAABQAAAAUAAAAFwAAABAAAAAUAAAAFAAAABcAAAAYAAAAFAAAABQAAAAWAAAAFAAAABQAAAAUAAAAFgAAABQAAAAUAAAAFAAAAB4AAAATAAAAHAAAAB0AAAArAAAAFAAAABMAAAATAAAAHwAAABQAAAAUAAAAEwAAABcAAAAQAAAAFAAAABQAAAAXAAAAEAAAABQAAAAUAAAAJAAAABkAAAAUAAAAFAAAABwAAAAUAAAAFAAAABMAAAAfAAAAFAAAABMAAAAUAAAAHgAAACkAAAAUAAAAKAAAABRzdGNvAAAAAAAAAAEAAAAwAAAAYnVkdGEAAABabWV0YQAAAAAAAAAhaGRscgAAAAAAAAAAbWRpcmFwcGwAAAAAAAAAAAAAAAAtaWxzdAAAACWpdG9vAAAAHWRhdGEAAAABAAAAAExhdmY1OC4yMC4xMDA=\" type=\"video/mp4\" />\n",
       "             </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "env = EnvironmentExploring(grid_size=size, max_time=T, temperature=0.3)\n",
    "agent = DQN_CNN(size, lr=.1, epsilon = 0.1, memory_size=2000, batch_size = 32,n_state=3)\n",
    "train_explore(agent, env, epochs_train, prefix='cnn_train_explore')\n",
    "HTML(display_videos('cnn_train_explore10.mp4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win/lose count 13.750000000000004/3.709999999999966. Average score (10.040000000000038)\n",
      "Win/lose count 14.700000000000005/4.699999999999968. Average score (10.020000000000039)\n",
      "Win/lose count 10.780000000000003/1.7900000000000014. Average score (9.676666666666693)\n",
      "Win/lose count 16.210000000000004/4.6799999999999695. Average score (10.140000000000027)\n",
      "Win/lose count 7.850000000000002/3.849999999999964. Average score (8.91200000000003)\n",
      "Win/lose count 6.370000000000002/1.8800000000000014. Average score (8.175000000000024)\n",
      "Win/lose count 3.920000000000001/1.9200000000000015. Average score (7.292857142857164)\n",
      "Win/lose count 2.9400000000000004/1.9400000000000015. Average score (6.506250000000017)\n",
      "Win/lose count 4.900000000000001/2.8999999999999817. Average score (6.005555555555572)\n",
      "Win/lose count 4.44/1.9200000000000015. Average score (5.657000000000015)\n",
      "Win/lose count 9.310000000000002/2.819999999999983. Average score (5.7327272727272875)\n",
      "Win/lose count 9.310000000000002/1.8200000000000014. Average score (5.879166666666681)\n",
      "Win/lose count 14.220000000000004/5.709999999999968. Average score (6.081538461538477)\n",
      "Win/lose count 4.900000000000001/2.9099999999999815. Average score (5.789285714285731)\n",
      "Win/lose count 1.96/1.9700000000000015. Average score (5.402666666666681)\n",
      "Win/lose count 13.230000000000004/1.7400000000000013. Average score (5.783125000000014)\n",
      "Win/lose count 9.830000000000002/2.7999999999999834. Average score (5.856470588235308)\n",
      "Win/lose count 10.290000000000003/1.8000000000000014. Average score (6.0027777777777915)\n",
      "Win/lose count 5.880000000000002/4.889999999999962. Average score (5.738947368421068)\n",
      "Win/lose count 4.410000000000001/1.9200000000000015. Average score (5.5765000000000144)\n",
      "Final score: 5.5765000000000144\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video alt=\"test\" controls>\n",
       "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
       "             </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation\n",
    "test(agent,env,epochs_test,prefix='cnn_test_explore')\n",
    "HTML(display_videos('cnn_test_explore10.mp4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "__BONUS question__ Use the expert DQN from the previous question to generate some winning games. Train a model that mimicks its behavior. Compare the performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
